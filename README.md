# AI_approval_officer

The process is for detecting check box or signed/chopped area from the given document

## png transformation

The png/jpg files are the most suitable for training.testing process. If your files are in pdf format, we provide the code pdf2image.py for transformation.

please ensure that your files be saved in folder "pdfs",and create an empty folder "images". The result will be saved in folder "images"

To run the code:
```python
python pdf2image.py
```
## Data cleansing

Before data cleansing, please make sure your files are sorted in the following format(here we will use our dataset for instance):
```
└─title

	└─images (folder including all images)
 
	└─labels (folder including all labels)
```
(for label files. please refer to the following section "Data labeling")

then, run the code(please change your root directory before running):
```python
python cleansing.py
```

The result should be in the following format. You can change the ratio of images in train/test/valid in the "cleansing.py"
```
└── yolov8_dataset

	└── train
 
		└── images (folder including all training images)
  
		└── labels (folder including all training labels)
  
	└── test
 
		└── images (folder including all testing images)
  
		└── labels (folder including all testing labels)
  
	└── valid
 
		└── images (folder including all testing images)
  
		└── labels (folder including all testing labels)
```
## Data labeling

You may get confused where the labeled files come from, here is the entire process:

1.Open the link of Make Sense: https://www.makesense.ai/ , click "Get Started" to start

2.Upload all images, and add the label(s) you need

3.Tedious labeling

4.Once labeling finished, click "Action" --> "export annotation", remember to select "yolo" format

Now the data labeling is done

### Optional Data labeling

You may find it tedious if your target identifying areas are fixed. Here is the explaination of txt files generated by Make Sense

The txt file may look like this:

0 0.366612 0.487222 0.049542 0.013667

^     ^        ^        ^        ^

|     |        |        |        |

|     |        |        |        |

type  x_center y_center width    height

The "type" represents for different label names, which connects to the yaml files,

Except for the first parameter, the last four parameters are all proportions. the bounding box can be calculated by the following code:

```python
corner1 = [int(10000 * (x_center - 0.5 * width)), int(10000 * (y_center - 0.5 * height))]
corner2 = [int(10000 * (x_center + 0.5 * width)), int(10000 * (y_center - 0.5 * height))]
corner3 = [int(10000 * (x_center - 0.5 * width)), int(10000 * (y_center + 0.5 * height))]
corner4 = [int(10000 * (x_center + 0.5 * width)), int(10000 * (y_center + 0.5 * height))]
```

Now you can diy your txt files for easier use.

## Writing yaml files

The last file of data processing is writing yaml files, which is not difficult. you can refer to "title.yaml" for quick understanding.

the root of yaml files is:

```
examples/ultralytics/ultralytics/datasets
```

## Train

```
# Build a new model from YAML and start training from scratch
yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640

# Start training from a pretrained *.pt model
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640

# Build a new model from YAML, transfer pretrained weights to it and start training
yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640

```

you should change "data" part to your own case, the results would be saved to runs/detect

## Predect
The pretrained model above is in runs/detect/train*, named "best.pt", you can predict results from your own model in the following command:

```
yolo task=detect mode=predict model=runs/detect/train*/weights/best.pt source=data/images device=0 save=True
```

please change the path of "source"
